---
title: "Regression Model Project"
author: "ahmed Bakhouz"
date: "Saturday, June 20, 2015"
output:
  word_document: default
  keep_md: yes
  toc: yes
  html_document: default
---

## Executive Summary


This paper explores the relationship between miles-per-gallon (MPG) and other variables in the mtcars data set. In particular, the analysis attempts to determine whether an automatic or manual transmission is better for MPG, and quantifies the MPG difference.


### 1. The data Description

The data set was extracted from the 1974 edition of Motor Trend US Magazine and it deals with 1973 - 1974 models. It consists of 32 observations on 11 variables:

- `mpg`: Miles per US gallon                        
- `cyl`: Number of cylinders    ()                 
- `disp`: Displacement (cubic inches)                    
- `hp`: Gross horsepower                         
- `drat`: Rear axle ratio                          
- `wt`: Weight (lb / 1000)                         
- `qsec`: 1 / 4 mile time                            
- `vs`: V/S                                      
- `am`: Transmission (0 = automatic, 1 = manual) 
- `gear`: Number of forward gears                  
- `carb`: Number of carburetors   

### 2. Analysis

### 2.1 Simple Linear Regression - lm(mpg ~ am, data = mtcars)

The exploratory analysis of the data is described in Appendix. Based on the exploratory analysis, we selected three models to explore the question posed by this report:

```{r}
data(mtcars)
n <- length(mtcars$mpg)
alpha <- 0.05
fit <- lm(mpg ~ am, data = mtcars)
coef(summary(fit))
```
The beta0 / intercept coefficient is mean MPG for cars with automatic transmissions; the beta1 / am coefficient is the mean increase in MPG for cars with manual transmissions (am = 1). The sum beta0 + beta1 is our mean MPG for cars with manual transmissions.

Using the output above, we can calculate a 95% confidence interval for beta1 (mean MPG difference) as follows:


```{r}
pe <- coef(summary(fit))["am", "Estimate"]
se <- coef(summary(fit))["am", "Std. Error"]
tstat <- qt(1 - alpha/2, n - 2)  # n - 2 for model with intercept and slope
pe + c(-1, 1) * (se * tstat)
```

The p-value of 2.850207410^{-4} for beta1 is small and the CI does not include zero, so we can reject null in favor of the alternative hypothesis that there is a significant difference in MPG between the two groups at alpha = 0.05.

### 2.2 Multiple Regression - lm(mpg ~ wt + qsec + am, data=mtcars)

The predictors wt (weight), qsec (1/4 mile time) and am (transmission type) were first selected in an automated fashion using the bestglm package. This set of predictors yields the highest adjusted R-squared. This result agrees with what you arrive at by following this logic:
1.Start with the predictor whose correlation with mpg is highest (wt);
2.Eliminate from the model variables that are highly correlated with wt;
3.Add the remaining predictor, qsec, which is nearly orthogonal to wt; and
4.Add our variable of interest, am, to see if it is a significant predictor.


```{r}
# fit a model using the regressors suggested by bestglm residual plot is in
# Appendix
bestfit <- lm(mpg ~ wt + qsec + am, data = mtcars)
coef(summary(bestfit))
```

Using the output above, we can calculate a 95% confidence interval for beta3 / am as follows:

```{r}
pe <- coef(summary(bestfit))["am", "Estimate"]
se <- coef(summary(bestfit))["am", "Std. Error"]
tstat <- qt(1 - alpha/2, n - 2)  # n - 2 for model with intercept and slope
pe + c(-1, 1) * (se * tstat)
```

The p-value of 0.0467155 for beta3 is small and the CI does not include zero, so we can reject null in favor of the alternative hypothesis that there is a significant difference in MPG between the two groups at alpha = 0.05.


### 2.3 Nested Model Testing: 

```{r, echo=FALSE}
# nested model testing of the model selected by bestglm
fit1 <- lm(mpg ~ wt, data = mtcars)
fit2 <- update(fit1, mpg ~ wt + qsec)
fit3 <- update(fit2, mpg ~ wt + qsec + am)
anova(fit1, fit2, fit3)
```


The nested model test demonstrated in Prof. Caffo's lecture confirms that all three regressors are significant.


Appendix - Exploratory Analysis and Visualizations


### 2.4 Correlations

```{r, echo=FALSE}
mtcars_vars <- mtcars[, c(1, 6, 7, 9)]
mar.orig <- par()$mar  # save the original values 
par(mar = c(1, 1, 1, 1))  # set your new values 
pairs(mtcars_vars, panel = panel.smooth, col = 9 + mtcars$wt)
```


```{r, echo=FALSE}
par(mar = mar.orig)  # put the original values back 
cor(mtcars_vars)
```

### 2.5 Histograms

Nothing remarkable here except perhaps in the weight / wt histogram. The Cadillac Fleetwood, Lincoln Continental and Chrysler Imperial are quite a bit heavier than other cars in the dataset.

```{r, echo=FALSE}
library(ggplot2)
library(gridExtra)
mpg_dist <- qplot(mtcars_vars$mpg, fill = I("red"))
wt_dist <- qplot(mtcars_vars$wt, fill = I("lightblue"))
qsec_dist <- qplot(mtcars_vars$qsec, fill = I("purple"))
am_dist <- qplot(mtcars_vars$am, fill = I("green"))
grid.arrange(mpg_dist, wt_dist, qsec_dist, am_dist, ncol = 2)
```


### 2.6 Homogeneity of Variance Assumption

Box plots, comparison of the standard deviations of MPG by transmission type, and Levene's test indicate that the assumption of homogeneity of variance is questionable.


###  Side-by-side box plots


```{r, echo=FALSE}
mtcars_vars <- mtcars[, c(1, 6, 7, 9)]
mar.orig <- par()$mar  # save the original values 
par(mar = c(2, 2, 2, 2))  # set your new values 
boxplot(mtcars_vars[mtcars_vars$am == 1, ]$mpg, mtcars_vars[mtcars_vars$am == 
    0, ]$mpg, names = c("Manual", "Automatic"))
```


```{r, echo=FALSE}
par(mar = mar.orig)  # put the original values back 
```

### 2.7 Standard Deviation of MPG by Transmission Type

```{r, echo=FALSE}
by(mtcars_vars$mpg, mtcars_vars$am, sd)
```


### 2.8 Levene's Test for Homogeneity of Variance

```{r, echo=FALSE}
library(car)
leveneTest(mpg ~ factor(am), data = mtcars_vars)
```

### 2.9 Residual Plot

There is a bit of a curve to the residual plot, so that it departs slightly from normality. The residuals for the Chrysler Imperial, Fiat 128, and Toyota Corolla are called out because they exert some influence on the shape of the curve.


```{r, echo=FALSE}
mar.orig <- par()$mar  # save the original values 
par(mar = c(2, 2, 2, 2))  # set your new values 
plot(bestfit, which = c(1:1))
```


```{r, echo=FALSE}
par(mar = mar.orig)  # put the original values back
```